{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, MetaEstimatorMixin\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "class GeneticAlgorithm(BaseEstimator, MetaEstimatorMixin):\n",
    "    def __init__(self, modelToTune, param_space, \n",
    "                 population_size=100, generations=1000, \n",
    "                 crossover_prob=0.8, mutation_prob=0.2,\n",
    "                 tournament_size=15, cv=5, scoring='accuracy',\n",
    "                 n_jobs=None, verbose=0, random_state=None):\n",
    "        self.modelToTune = modelToTune #actual model\n",
    "        self.param_space = param_space #parameters\n",
    "        self.population_size = population_size #initial population size\n",
    "        self.generations = generations #num of generations\n",
    "        self.crossover_prob = crossover_prob #probability (0,1) of crossover\n",
    "        self.mutation_prob = mutation_prob #probability (0,1) of mutation\n",
    "        self.tournament_size = tournament_size #basically how large of\n",
    "        self.cv = cv #cross val fold for fitness\n",
    "        self.scoring = scoring #what method to get fitness from\n",
    "        self.n_jobs = n_jobs #for parallelism \n",
    "        self.verbose = verbose \n",
    "        self.random_state = random_state\n",
    "        self.best_params_ = [] #list of best parameters\n",
    "        self.best_score_ = None #best score so far\n",
    "        self.history_ = []\n",
    "        \n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "            random.seed(random_state)\n",
    "    \n",
    "    def _initialize_individual(self):\n",
    "        individual = {}\n",
    "        for param, values in self.param_space.items(): #selecting from discreet options\n",
    "            if isinstance(values, (list, tuple, range)): \n",
    "                individual[param] = random.choice(values)\n",
    "            elif callable(values):\n",
    "                individual[param] = values()\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid parameter range for {param}\") #something went wrong\n",
    "        return individual\n",
    "    \n",
    "    def _initialize_population(self):\n",
    "        return [self._initialize_individual() for _ in range(self.population_size)]\n",
    "    \n",
    "    def _evaluate_individual(self, individual, X, y):\n",
    "        model = self.modelToTune.set_params(**individual)\n",
    "        scores = cross_val_score(model, X, y, cv=self.cv, \n",
    "                                scoring=self.scoring, n_jobs=self.n_jobs)\n",
    "        return np.mean(scores)\n",
    "    \n",
    "    def _tournament_selection(self, population, fitness_scores):\n",
    "        #I set up a tournament approach to selecing parents\n",
    "        #makes models from a sample undergo a tournament, where we use the best one as the parent\n",
    "\n",
    "        selected = random.sample(range(len(population)), self.tournament_size)\n",
    "        selected_fitness = [fitness_scores[i] for i in selected]\n",
    "        winner = selected[np.argmax(selected_fitness)]\n",
    "        return population[winner]\n",
    "    \n",
    "    def _crossover(self, parent1, parent2):\n",
    "        #actual crossover\n",
    "        child = {}\n",
    "        for param in parent1.keys():\n",
    "            if random.random() < 0.5:\n",
    "                child[param] = parent1[param]\n",
    "            else:\n",
    "                child[param] = parent2[param]\n",
    "        return child\n",
    "    \n",
    "    def _mutate(self, individual):\n",
    "        #copy the OG\n",
    "        mutated = individual.copy()\n",
    "        #randomly select a parameter to mutate\n",
    "        param_to_mutate = random.choice(list(self.param_space.keys()))\n",
    "        \n",
    "        #if we have a range to choose from, we select a random value from the range\n",
    "        if isinstance(self.param_space[param_to_mutate], (list, tuple, range)):\n",
    "            current_value = mutated[param_to_mutate]\n",
    "            possible_values = [v for v in self.param_space[param_to_mutate] if v != current_value]\n",
    "            if possible_values:\n",
    "                mutated[param_to_mutate] = random.choice(possible_values)\n",
    "                        #otherwise we just select one of out options\n",
    "\n",
    "        elif callable(self.param_space[param_to_mutate]):\n",
    "            mutated[param_to_mutate] = self.param_space[param_to_mutate]()\n",
    "        \n",
    "        return mutated\n",
    "    \n",
    "    def run(self, X, y):\n",
    "        population = self._initialize_population()\n",
    "        \n",
    "        \n",
    "        for gen in range(self.generations):\n",
    "            #get fitness for all models in gen\n",
    "            fitness_scores = [self._evaluate_individual(ind, X, y) for ind in population]\n",
    "            #get the best one by id\n",
    "            best_idx = np.argmax(fitness_scores)\n",
    "            current_best_score = fitness_scores[best_idx]\n",
    "            current_best_params = population[best_idx]\n",
    "            \n",
    "            #set the new best score if it wins\n",
    "            if self.best_score_ is None or current_best_score > self.best_score_:\n",
    "                self.best_score_ = current_best_score\n",
    "                self.best_params_ = current_best_params.copy()\n",
    "            \n",
    "            self.history_.append({\n",
    "                'generation': gen,\n",
    "                'best_score': self.best_score_,\n",
    "                'avg_score': np.mean(fitness_scores),\n",
    "                'best_params': self.best_params_\n",
    "            })\n",
    "            \n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Generation {gen+1}/{self.generations} - Best: {self.best_score_:.4f} - Avg: {np.mean(fitness_scores):.4f}\")\n",
    "            \n",
    "            #make a new population\n",
    "            new_population = []\n",
    "            #add the best parameters\n",
    "            new_population.append(self.best_params_)\n",
    "            \n",
    "\n",
    "            while len(new_population) < self.population_size:\n",
    "                parent1 = self._tournament_selection(population, fitness_scores)\n",
    "                parent2 = self._tournament_selection(population, fitness_scores)\n",
    "                \n",
    "                #if crossover selected, our child is made up of both parent\n",
    "                if random.random() < self.crossover_prob:\n",
    "                    child = self._crossover(parent1, parent2)\n",
    "                else:\n",
    "                #othterwise we just carry one of the two parents into the next gen\n",
    "                    child = random.choice([parent1, parent2])\n",
    "                \n",
    "                #and we can mutate\n",
    "                if random.random() < self.mutation_prob:\n",
    "                    child = self._mutate(child)\n",
    "                \n",
    "                new_population.append(child)\n",
    "            \n",
    "            population = new_population\n",
    "        \n",
    "        self.bestModel = self.modelToTune.set_params(**self.best_params_)\n",
    "        self.bestModel.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "feature_names = diabetes.feature_names\n",
    "\n",
    "# Convert to binary classification\n",
    "y_binary = np.where(y > np.median(y), 1, 0)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_binary, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define parameter space\n",
    "param_space = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(1, 20),\n",
    "    'min_samples_split': range(2, 20),\n",
    "    'min_samples_leaf': range(1, 20),\n",
    "    'max_features': [None, 'sqrt', 'log2', 0.5, 0.7]\n",
    "}\n",
    "\n",
    "\n",
    "# Create and run genetic algorithm\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ga = GeneticAlgorithm(\n",
    "    modelToTune=dt,\n",
    "    param_space=param_space,\n",
    "    population_size=50,\n",
    "    generations=10,\n",
    "    crossover_prob=0.85,\n",
    "    mutation_prob=0.4,\n",
    "    tournament_size=10,\n",
    "    cv=10,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ga.run(X_train, y_train)\n",
    "\n",
    "# Results\n",
    "print(\"\\nBest parameters found:\")\n",
    "print(ga.best_params_)\n",
    "print(f\"\\nBest cross-validation score: {ga.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = ga.bestModel.score(X_test, y_test)\n",
    "print(f\"\\nTest set accuracy: {test_score:.4f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "if hasattr(ga.bestModel, 'feature_importances_'):\n",
    "    print(\"\\nFeature importances:\")\n",
    "    for name, importance in zip(feature_names, ga.bestModel.feature_importances_):\n",
    "        print(f\"{name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Binarize targets\n",
    "y_train_binary = np.where(y_train > np.median(y_train), 1, 0)\n",
    "y_test_binary = np.where(y_test > np.median(y_test), 1, 0)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Logistic Regression parameter space for both GA and GridSearch\n",
    "logreg_param_space = {\n",
    "    'C': list(np.logspace(-3, 3, 20)),\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': range(1000, 10001, 1000),  # Now a range from 1000 to 10000 in steps of 1000\n",
    "    'tol': [1e-4, 1e-5],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# 1. Genetic Algorithm Implementation\n",
    "ga_logreg = GeneticAlgorithm(\n",
    "    modelToTune=LogisticRegression(random_state=42),\n",
    "    param_space=logreg_param_space,\n",
    "    population_size=100,\n",
    "    generations=50,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Running Genetic Algorithm...\")\n",
    "ga_logreg.run(X_train, y_train_binary)\n",
    "\n",
    "# 2. Grid Search Implementation\n",
    "print(\"\\nRunning Grid Search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=LogisticRegression(random_state=42),\n",
    "    param_grid=logreg_param_space,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "grid_search.fit(X_train, y_train_binary)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nComparison of Results:\")\n",
    "print(\"\\nGenetic Algorithm:\")\n",
    "print(f\"Best Parameters: {ga_logreg.best_params_}\")\n",
    "print(f\"Best CV Accuracy: {ga_logreg.best_score_:.4f}\")\n",
    "print(f\"Test Accuracy: {ga_logreg.bestModel.score(X_test, y_test_binary):.4f}\")\n",
    "\n",
    "print(\"\\nGrid Search:\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV Accuracy: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Accuracy: {grid_search.score(X_test, y_test_binary):.4f}\")\n",
    "\n",
    "# Feature coefficients from best model\n",
    "print(\"\\nFeature Coefficients (Genetic Algorithm):\")\n",
    "for name, coef in zip(diabetes.feature_names, ga_logreg.bestModel.coef_[0]):\n",
    "    print(f\"{name}: {coef:.4f}\")\n",
    "\n",
    "print(\"\\nFeature Coefficients (Grid Search):\")\n",
    "for name, coef in zip(diabetes.feature_names, grid_search.best_estimator_.coef_[0]):\n",
    "    print(f\"{name}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression parameter space for both GA and GridSearch\n",
    "\n",
    "# Corrected Logistic Regression parameter space\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Configure warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"l1_ratio parameter is only used when penalty is 'elasticnet'\")\n",
    "\n",
    "# Optimized Parameter Space with solver-penalty mapping\n",
    "logreg_param_space = {\n",
    "    'C': list(np.logspace(-4, 4, 20)),\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': {  # Now properly structured\n",
    "        'l1': ['liblinear', 'saga'],\n",
    "        'l2': ['lbfgs', 'newton-cg', 'sag', 'saga', 'liblinear']\n",
    "    },\n",
    "    'max_iter': list(range(100, 2001, 100)),\n",
    "    'tol': [1e-4, 1e-5, 1e-6],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "class PenaltyAwareLogisticGA(GeneticAlgorithm):\n",
    "    def _initialize_individual(self):\n",
    "        \"\"\"Initialize an individual with guaranteed valid penalty-solver pairs\"\"\"\n",
    "        individual = {}\n",
    "        \n",
    "        # 1. First select penalty\n",
    "        individual['penalty'] = random.choice(self.param_space['penalty'])\n",
    "        \n",
    "        # 2. Select compatible solver\n",
    "        individual['solver'] = random.choice(self.param_space['solver'][individual['penalty']])\n",
    "        \n",
    "        # 3. Fill remaining parameters\n",
    "        for param in ['C', 'max_iter', 'tol', 'class_weight', 'fit_intercept']:\n",
    "            individual[param] = random.choice(self.param_space[param])\n",
    "            \n",
    "        return individual\n",
    "    \n",
    "    def _crossover(self, parent1, parent2):\n",
    "        \"\"\"Crossover with penalty-solver compatibility enforcement\"\"\"\n",
    "        child = {}\n",
    "        \n",
    "        # Inherit penalty (50/50 chance)\n",
    "        child['penalty'] = parent1['penalty'] if random.random() < 0.5 else parent2['penalty']\n",
    "        \n",
    "        # Select compatible solver from the chosen parent\n",
    "        if child['penalty'] == parent1['penalty']:\n",
    "            child['solver'] = parent1['solver']\n",
    "        else:\n",
    "            child['solver'] = parent2['solver']\n",
    "        \n",
    "        # Optionally: Allow solver mutation during crossover\n",
    "        if random.random() < 0.2:  # 20% chance to explore new solver\n",
    "            child['solver'] = random.choice(\n",
    "                [s for s in self.param_space['solver'][child['penalty']] if s != child['solver']]\n",
    "            )\n",
    "        \n",
    "        # Crossover other parameters\n",
    "        for param in ['C', 'max_iter', 'tol', 'class_weight', 'fit_intercept']:\n",
    "            child[param] = parent1[param] if random.random() < 0.5 else parent2[param]\n",
    "            \n",
    "        return child\n",
    "    \n",
    "    def _mutate(self, individual):\n",
    "        \"\"\"Mutation that maintains valid configurations\"\"\"\n",
    "        mutated = individual.copy()\n",
    "        \n",
    "        # Decide which parameter to mutate\n",
    "        param_to_mutate = random.choice(list(self.param_space.keys()))\n",
    "        \n",
    "        # Special handling for penalty and solver\n",
    "        if param_to_mutate == 'penalty':\n",
    "            new_penalty = random.choice([p for p in self.param_space['penalty'] if p != mutated['penalty']])\n",
    "            mutated['penalty'] = new_penalty\n",
    "            # Must also change solver to be compatible\n",
    "            mutated['solver'] = random.choice(self.param_space['solver'][new_penalty])\n",
    "            \n",
    "        elif param_to_mutate == 'solver':\n",
    "            # Only mutate to other compatible solvers\n",
    "            compatible_solvers = [s for s in self.param_space['solver'][mutated['penalty']] \n",
    "                                if s != mutated['solver']]\n",
    "            if compatible_solvers:\n",
    "                mutated['solver'] = random.choice(compatible_solvers)\n",
    "                \n",
    "        else:  # Regular parameters\n",
    "            current_val = mutated[param_to_mutate]\n",
    "            options = [v for v in self.param_space[param_to_mutate] if v != current_val]\n",
    "            if options:\n",
    "                mutated[param_to_mutate] = random.choice(options)\n",
    "        \n",
    "        return mutated\n",
    "    \n",
    "    def _evaluate_individual(self, individual, X, y):\n",
    "        \"\"\"Safety check before evaluation\"\"\"\n",
    "        # Double-check compatibility\n",
    "        if individual['solver'] not in self.param_space['solver'][individual['penalty']]:\n",
    "            return -np.inf  # Invalid individual gets worst possible score\n",
    "            \n",
    "        try:\n",
    "            model = LogisticRegression(**individual, random_state=self.random_state)\n",
    "            scores = cross_val_score(model, X, y, cv=self.cv, scoring=self.scoring)\n",
    "            return np.mean(scores)\n",
    "        except:\n",
    "            return -np.inf\n",
    "\n",
    "# Initialize and run\n",
    "ga_logreg = PenaltyAwareLogisticGA(\n",
    "    modelToTune=LogisticRegression(random_state=42),\n",
    "    param_space=logreg_param_space,\n",
    "    population_size=50,\n",
    "    generations=30,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Running Penalty-Aware Genetic Algorithm...\")\n",
    "ga_logreg.run(X_train, y_train_binary)\n",
    "\n",
    "\n",
    "# Results\n",
    "print(\"\\n=== OPTIMIZATION RESULTS ===\")\n",
    "print(f\"Best Parameters: {ga_logreg.best_params_}\")\n",
    "print(f\"Best CV Accuracy: {ga_logreg.best_score_:.4f}\")\n",
    "print(f\"Test Accuracy: {ga_logreg.bestModel.score(X_test, y_test_binary):.4f}\")\n",
    "\n",
    "# Feature coefficients\n",
    "print(\"\\nFeature Coefficients:\")\n",
    "for name, coef in zip(diabetes.feature_names, ga_logreg.bestModel.coef_[0]):\n",
    "    print(f\"{name:>8}: {coef:>10.4f}\")\n",
    "\n",
    "# =============================================\n",
    "# ADDED GRID SEARCH IMPLEMENTATION FOR COMPARISON\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nPreparing Grid Search...\")\n",
    "\n",
    "# Convert parameter space to GridSearchCV compatible format\n",
    "param_grid = [\n",
    "    {\n",
    "        'penalty': ['l1'],\n",
    "        'solver': logreg_param_space['solver']['l1'],\n",
    "        'C': logreg_param_space['C'],\n",
    "        'max_iter': logreg_param_space['max_iter'],\n",
    "        'tol': logreg_param_space['tol'],\n",
    "        'class_weight': logreg_param_space['class_weight'],\n",
    "        'fit_intercept': logreg_param_space['fit_intercept']\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l2'],\n",
    "        'solver': logreg_param_space['solver']['l2'],\n",
    "        'C': logreg_param_space['C'],\n",
    "        'max_iter': logreg_param_space['max_iter'],\n",
    "        'tol': logreg_param_space['tol'],\n",
    "        'class_weight': logreg_param_space['class_weight'],\n",
    "        'fit_intercept': logreg_param_space['fit_intercept']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=LogisticRegression(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Run Grid Search\n",
    "print(\"\\nRunning Grid Search CV...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train_binary)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "# =============================================\n",
    "# COMPARISON OF RESULTS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n=== FINAL COMPARISON ===\")\n",
    "\n",
    "# GA Results\n",
    "print(\"\\nGenetic Algorithm Results:\")\n",
    "print(f\"Best Parameters: {ga_logreg.best_params_}\")\n",
    "print(f\"Best CV Accuracy: {ga_logreg.best_score_:.4f}\")\n",
    "print(f\"Test Accuracy: {ga_logreg.bestModel.score(X_test, y_test_binary):.4f}\")\n",
    "\n",
    "# Grid Search Results\n",
    "print(\"\\nGrid Search Results:\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV Accuracy: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Accuracy: {grid_search.score(X_test, y_test_binary):.4f}\")\n",
    "\n",
    "# Timing Comparison\n",
    "ga_time = ga_logreg.total_time_elapsed_  # Assuming your GA class tracks this\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(f\"GA Time: {ga_time:.2f} seconds\")\n",
    "print(f\"Grid Search Time: {grid_time:.2f} seconds\")\n",
    "print(f\"Speed Ratio: {grid_time/max(ga_time,0.001):.1f}x\")\n",
    "\n",
    "# Feature Coefficients Comparison\n",
    "print(\"\\nFeature Coefficients Comparison:\")\n",
    "print(\"{:<10} {:<15} {:<15}\".format('Feature', 'GA Coef', 'GridSearch Coef'))\n",
    "for name, ga_coef, gs_coef in zip(diabetes.feature_names,\n",
    "                                 ga_logreg.bestModel.coef_[0],\n",
    "                                 grid_search.best_estimator_.coef_[0]):\n",
    "    print(\"{:<10} {:<15.4f} {:<15.4f}\".format(name, ga_coef, gs_coef))\n",
    "\n",
    "# Additional Grid Search Info\n",
    "print(\"\\nGrid Search Details:\")\n",
    "print(f\"Total combinations tested: {len(grid_search.cv_results_['params'])}\")\n",
    "print(f\"Best iteration: {grid_search.best_index_}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
